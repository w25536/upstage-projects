import os
import asyncio
import json
import glob
import torch
import chromadb
import requests
from datetime import datetime
from dotenv import load_dotenv
from crewai import Agent, Task, Crew
from crewai.llm import LLM
from bs4 import BeautifulSoup
from functools import partial   ### ğŸ”§ FIX: partial import (trace ì €ì¥ì‹œ íšŒì‚¬ëª… ì „ë‹¬)

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter

load_dotenv()

# =================================================================
# 1. RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° í•¨ìˆ˜ ì •ì˜
# =================================================================

# --- ê²½ë¡œ ì„¤ì • ---
PROPOSAL_DIR = "./proposal"
RFP_PATH = "./RFP/ìˆ˜í˜‘_rfp.pdf"
OUTPUT_DIR = "./output"
EVALUATION_CRITERIA_PATH = "./standard/evaluation_criteria.md"
CHROMA_PERSIST_DIR = "./chroma_db_html_parsed"
INTERNAL_DATA_DIR = "./internal_data"  # ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ (ê¸°ìˆ ìŠ¤íƒ, ë‹´ë‹¹ì, ë§ˆì´ê·¸ë ˆì´ì…˜, ì¥ì• ì´ë ¥ ë“±)
UPSTAGE_API_KEY = os.getenv("UPSTAGE_API_KEY")

# --- ì „ì—­ ë³€ìˆ˜ ---
unified_vectorstore = None
company_map = {}   # {"Aì‚¬_ì œì•ˆì„œ": "Aì‚¬"} í˜•íƒœ
llm = None         ### ğŸ”§ FIX: llm ì „ì—­ë³€ìˆ˜ ëª…ì‹œì ìœ¼ë¡œ ì„ ì–¸

# =================================================================
# HTML ì²­í‚¹ í•¨ìˆ˜ (RAG_pipeline.ipynbì—ì„œ ê°€ì ¸ì˜´)
# =================================================================

def split_text_by_length(text, max_length, overlap):
    """í…ìŠ¤íŠ¸ê°€ ê¸¸ ê²½ìš° ì§€ì •ëœ í¬ê¸°ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    if len(text) <= max_length:
        return [text]
    chunks = []
    start = 0
    while start < len(text):
        end = start + max_length
        chunks.append(text[start:end])
        start += max_length - overlap
    return chunks

def chunk_html_recursively(html_content, proposal_id, max_chunk_size=1000, chunk_overlap=100):
    """HTMLì„ ì œëª©(h íƒœê·¸) ê³„ì¸µ êµ¬ì¡°ì— ë”°ë¼ ì•ˆì •ì ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html_content, 'html.parser')
    chunks = []
    chunk_index = 0
    
    header_tags = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']
    all_headers = soup.find_all(header_tags)

    # í—¤ë”ê°€ ì‹œì‘ë˜ê¸° ì „ì˜ í…ìŠ¤íŠ¸ë„ ì²˜ë¦¬
    if all_headers:
        first_header = all_headers[0]
        text_before_first_header = ""
        for elem in first_header.find_previous_siblings():
            text_before_first_header = elem.get_text(separator=' ', strip=True) + " " + text_before_first_header
        
        if text_before_first_header.strip():
            text_chunks = split_text_by_length(text_before_first_header.strip(), max_chunk_size, chunk_overlap)
            for text_chunk in text_chunks:
                chunks.append({
                    "proposal_id": proposal_id,
                    "source_id": f"{proposal_id}_chunk_{chunk_index}",
                    "heading_context": "ì„œë¬¸",
                    "original_text": text_chunk
                })
                chunk_index += 1

    # í—¤ë” ê¸°ë°˜ìœ¼ë¡œ ì„¹ì…˜ ë¶„í• 
    for i, header in enumerate(all_headers):
        section_text_parts = [header.get_text(separator=' ', strip=True)]
        
        for sibling in header.find_next_siblings():
            if sibling.name in header_tags:
                break
            section_text_parts.append(sibling.get_text(separator=' ', strip=True))

        full_section_text = ' '.join(filter(None, section_text_parts))
        if not full_section_text.strip():
            continue

        parent_headers = [h.get_text(strip=True) for h in header.find_parents(header_tags)]
        parent_headers.reverse()
        heading_context = parent_headers + [header.get_text(strip=True)]

        text_chunks = split_text_by_length(full_section_text, max_chunk_size, chunk_overlap)
        for text_chunk in text_chunks:
            chunks.append({
                "proposal_id": proposal_id,
                "source_id": f"{proposal_id}_chunk_{chunk_index}",
                "heading_context": " > ".join(heading_context),
                "original_text": text_chunk
            })
            chunk_index += 1
            
    # í—¤ë”ê°€ ì „í˜€ ì—†ëŠ” ê²½ìš° body ì „ì²´ë¥¼ ì²˜ë¦¬
    if not all_headers and soup.body:
        full_text = soup.body.get_text(separator=' ', strip=True)
        if full_text:
            text_chunks = split_text_by_length(full_text, max_chunk_size, chunk_overlap)
            for text_chunk in text_chunks:
                chunks.append({
                    "proposal_id": proposal_id,
                    "source_id": f"{proposal_id}_chunk_{chunk_index}",
                    "heading_context": "ë³¸ë¬¸",
                    "original_text": text_chunk
                })
                chunk_index += 1
                
    return chunks

# =================================================================
# ì‚¬ë‚´ ì •ë³´ ë¡œë” í•¨ìˆ˜
# =================================================================

def load_all_internal_data_simple(internal_data_dir):
    """
    ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ê°„ë‹¨í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    ì •í˜•/ë¹„ì •í˜• êµ¬ë¶„ ì—†ì´ ëª¨ë“  .txt íŒŒì¼ì„ Documentë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤.
    ê° íŒŒì¼ì€ í†µì§¸ë¡œ í•˜ë‚˜ì˜ Documentê°€ ë˜ë©°, íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì…ì„ ìë™ ì¶”ë¡ í•©ë‹ˆë‹¤.

    Args:
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        list[Document]: ëª¨ë“  ì‚¬ë‚´ ì •ë³´ Document ë¦¬ìŠ¤íŠ¸
    """
    all_internal_docs = []

    if not os.path.exists(internal_data_dir):
        print(f"  âš  ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {internal_data_dir}")
        return all_internal_docs

    print(f"\n[ì‚¬ë‚´ ì •ë³´ ë¡œë“œ ì‹œì‘: {internal_data_dir}]")

    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .txt íŒŒì¼ ê²€ìƒ‰
    internal_files = glob.glob(os.path.join(internal_data_dir, "*.txt"))

    for file_path in internal_files:
        filename = os.path.basename(file_path)

        try:
            # íŒŒì¼ ë‚´ìš© ì „ì²´ë¥¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
            if 'tech_stack' in filename.lower():
                doc_type = "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ"
            elif 'contact' in filename.lower():
                doc_type = "ì‚¬ë‚´_ë‹´ë‹¹ì"
            elif 'migration' in filename.lower():
                doc_type = "ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜"
            elif 'incident' in filename.lower():
                doc_type = "ì‚¬ë‚´_ì¥ì• ì´ë ¥"
            else:
                doc_type = "ì‚¬ë‚´_ê¸°íƒ€"

            # Document ìƒì„± (íŒŒì¼ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ Documentë¡œ)
            doc = Document(
                page_content=content,
                metadata={
                    "doc_type": doc_type,
                    "source_file": filename
                }
            )
            all_internal_docs.append(doc)
            print(f"  âœ“ [{doc_type}] {filename} ë¡œë“œ ì™„ë£Œ ({len(content)} ì)")

        except Exception as e:
            print(f"  âœ— {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    print(f"\nâœ… ì´ {len(all_internal_docs)}ê°œì˜ ì‚¬ë‚´ ì •ë³´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ\n")
    return all_internal_docs

# =================================================================
# RAG ì´ˆê¸°í™” í•¨ìˆ˜
# =================================================================

def initialize_rag_components():
    """RAGì— í•„ìš”í•œ ì„ë² ë”© ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    if torch.cuda.is_available():
        device = "cuda"
        torch.cuda.set_device(0)
        print(f"INFO: CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = "cpu"
        print("WARNING: CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    return embedding_model

def create_unified_vectorstore(proposal_files, rfp_path, embedding_model):
    """ëª¨ë“  ì œì•ˆì„œì™€ RFP ë¬¸ì„œë¥¼ ë‹¨ì¼ ë²¡í„° ìŠ¤í† ì–´ë¡œ ë³€í™˜"""
    print(f"\n--- [RAG Setup] í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤ ---")
    
    all_chunked_data = []
    
    # 1. RFP(PDF) ì²˜ë¦¬ - Upstage API ì‚¬ìš©
    print("\n[RFP ì²˜ë¦¬]")
    try:
        rfp_id = os.path.splitext(os.path.basename(rfp_path))[0]
        url = "https://api.upstage.ai/v1/document-digitization"
        headers = {"Authorization": f"Bearer {UPSTAGE_API_KEY}"}
        
        with open(rfp_path, "rb") as f:
            files = {"document": f}
            data = {"ocr": "force", "model": "document-parse"}
            response = requests.post(url, headers=headers, files=files, data=data)
            response.raise_for_status()
            html_from_api = response.json().get("content", {}).get("html", "")
            
        if html_from_api:
            rfp_chunks = chunk_html_recursively(html_from_api, rfp_id)
            all_chunked_data.extend(rfp_chunks)
            print(f"  âœ“ RFP '{rfp_id}' ì²˜ë¦¬ ì™„ë£Œ: {len(rfp_chunks)}ê°œ ì²­í¬ ìƒì„±")
    except Exception as e:
        print(f"  âœ— RFP ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    # 2. ì œì•ˆì„œ(HTML) ì²˜ë¦¬
    print("\n[ì œì•ˆì„œ ì²˜ë¦¬]")
    for file_path in proposal_files:
        try:
            proposal_id = os.path.splitext(os.path.basename(file_path))[0]
            with open(file_path, 'r', encoding='utf-8') as f:
                html_content = f.read()

            proposal_chunks = chunk_html_recursively(html_content, proposal_id)
            all_chunked_data.extend(proposal_chunks)
            print(f"  âœ“ ì œì•ˆì„œ '{proposal_id}' ì²˜ë¦¬ ì™„ë£Œ: {len(proposal_chunks)}ê°œ ì²­í¬ ìƒì„±")
        except Exception as e:
            print(f"  âœ— ì œì•ˆì„œ '{os.path.basename(file_path)}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")

    # 3. ì‚¬ë‚´ ì •ë³´ ë¡œë“œ (ê¸°ìˆ ìŠ¤íƒ, ë‹´ë‹¹ì, ë§ˆì´ê·¸ë ˆì´ì…˜ ì´ë ¥, ì¥ì•  ì´ë ¥ ë“±)
    print("\n[ì‚¬ë‚´ ì •ë³´ ì²˜ë¦¬]")
    internal_docs = load_all_internal_data_simple(INTERNAL_DATA_DIR)

    # ì‚¬ë‚´ ì •ë³´ë¥¼ ì²­í¬ í˜•íƒœë¡œ ë³€í™˜ (ê¸°ì¡´ êµ¬ì¡°ì™€ í†µì¼)
    for idx, doc in enumerate(internal_docs):
        doc_type = doc.metadata.get("doc_type", "ì‚¬ë‚´_ê¸°íƒ€")
        source_file = doc.metadata.get("source_file", "unknown")

        # ë‚´ìš©ì´ ê¸´ ê²½ìš° ì ì ˆíˆ ë¶„í•  (max 1000ì)
        content = doc.page_content
        if len(content) > 1000:
            # 1000ì ë‹¨ìœ„ë¡œ ë¶„í•  (ì˜¤ë²„ë© 100ì)
            chunks = split_text_by_length(content, 1000, 100)
            for chunk_idx, chunk in enumerate(chunks):
                all_chunked_data.append({
                    "proposal_id": f"internal_{doc_type}",
                    "source_id": f"internal_{source_file}_{idx}_{chunk_idx}",
                    "heading_context": f"{doc_type} > {source_file}",
                    "original_text": chunk
                })
        else:
            all_chunked_data.append({
                "proposal_id": f"internal_{doc_type}",
                "source_id": f"internal_{source_file}_{idx}",
                "heading_context": f"{doc_type} > {source_file}",
                "original_text": content
            })

    print(f"  âœ“ ì‚¬ë‚´ ì •ë³´ {len(internal_docs)}ê°œ íŒŒì¼ì„ ì²­í¬ë¡œ ë³€í™˜ ì™„ë£Œ")

    if not all_chunked_data:
        print("\në²¡í„° DBì— ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return None

    # 3. Document ê°ì²´ë¡œ ë³€í™˜
    documents = [
        Document(
            page_content=chunk["original_text"],
            metadata={
                "proposal_id": chunk["proposal_id"],
                "source_id": chunk["source_id"],
                "heading_context": chunk["heading_context"]
            }
        ) for chunk in all_chunked_data
    ]
    print(f"\nâœ“ ì´ {len(documents)}ê°œì˜ ì²­í¬ë¥¼ Document ê°ì²´ë¡œ ë³€í™˜í–ˆìŠµë‹ˆë‹¤.")

    # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
    print("\n[ë²¡í„°í™” ë° ì €ì¥]")
    vectorstore = Chroma.from_documents(
        documents=documents,
        embedding=embedding_model,
        persist_directory=CHROMA_PERSIST_DIR
    )
    print(f"  âœ“ í†µí•© ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ! ({CHROMA_PERSIST_DIR})")
    return vectorstore

def get_context_for_category(proposal_file, category_keywords):
    """ëŒ€ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    global unified_vectorstore
    if unified_vectorstore is None:
        return "ì˜¤ë¥˜: ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    print(f"INFO: RAG ê²€ìƒ‰ ì‹¤í–‰ -> ì œì•ˆì„œ: '{proposal_file}', ëŒ€ë¶„ë¥˜: '{category_keywords}'")
    
    # ëŒ€ë¶„ë¥˜ ê´€ë ¨ ë‚´ìš©ì„ ë” ë§ì´ ê°€ì ¸ì˜´
    results = unified_vectorstore.similarity_search(
        query=category_keywords,
        k=10,  # ë” ë§ì€ ì²­í¬ë¥¼ ê²€ìƒ‰ (ì œí•œ ì œê±°)
        filter={"proposal_id": proposal_file}
    )
    
    if not results:
        return "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
    # ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í¬í•¨ (ê¸¸ì´ ì œí•œ ì œê±°)
    context = "\n\n---\n\n".join([doc.page_content for doc in results])
    
    print(f"INFO: ì´ {len(results)}ê°œ ì²­í¬, {len(context)}ìì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
    
    return context

def load_evaluation_criteria_as_text(criteria_path):
    """í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¡œë“œí•©ë‹ˆë‹¤ (íŒŒì‹± ì—†ìŒ)."""
    if not os.path.exists(criteria_path):
        print(f"WARNING: í‰ê°€ ê¸°ì¤€í‘œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {criteria_path}")
        return ""
    
    print(f"INFO: í‰ê°€ ê¸°ì¤€í‘œ ë¡œë”©: {criteria_path}")
    
    with open(criteria_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"INFO: í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤ ({len(content)} ì)")
    return content

def save_evaluation_report(proposal_name, report_content):
    """ì œì•ˆì„œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    proposal_base_name = os.path.splitext(proposal_name)[0]
    filename = f"{proposal_base_name}_evaluation_report_{timestamp}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("ì œì•ˆì„œ í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(f"ì œì•ˆì„œëª…: {proposal_name}\n")
        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("="*80 + "\n")
        f.write("ìµœì¢… í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(report_content)
        f.write("\n\n" + "="*80 + "\n")
        f.write("ë³´ê³ ì„œ ë\n")
        f.write("="*80 + "\n")
    
    print(f"[SAVED] í‰ê°€ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    return filepath

# =================================================================
# LLM ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
# =================================================================

def get_llm_model():
    """í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ LLM ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    global llm
    if llm is not None:
        return llm

    model_type = os.getenv('LLM_TYPE', 'local').lower()
    
    if model_type == 'local':
        model_name = os.getenv('LOCAL_MODEL_NAME', 'llama-blossom')
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        print(f"INFO: ë¡œì»¬ LLM ì‚¬ìš© - ëª¨ë¸: {model_name}, URL: {base_url}")
        return LLM(model=f"ollama/{model_name}", base_url=base_url)
    
    elif model_type == 'huggingface':
        model_name = os.getenv('HF_MODEL_NAME', 'meta-llama/Meta-Llama-3-8B-Instruct')
        api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')
        if not api_key:
            raise ValueError("HUGGINGFACEHUB_API_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"INFO: HuggingFace LLM ì‚¬ìš© - ëª¨ë¸: {model_name}")
        return LLM(model=f"huggingface/{model_name}", api_key=api_key)
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM íƒ€ì…ì…ë‹ˆë‹¤: {model_type}")
    
    return llm

async def main():
    print("## LLM ì£¼ë„í˜• ë™ì  Agent ìƒì„± ë° í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    global unified_vectorstore, company_map
    proposal_files = glob.glob(os.path.join(PROPOSAL_DIR, "*.html"))
    
    if not proposal_files or not os.path.exists(RFP_PATH):
        print("ì˜¤ë¥˜: ì œì•ˆì„œ ë˜ëŠ” RFP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        return
    
    # íšŒì‚¬ëª… ë§¤í•‘ í…Œì´ë¸” ìƒì„±
    company_map = {}
    for file_path in proposal_files:
        proposal_name = os.path.splitext(os.path.basename(file_path))[0]
        # íŒŒì¼ëª…ì—ì„œ íšŒì‚¬ëª… ì¶”ì¶œ (ì˜ˆ: "Aì‚¬_ì œì•ˆì„œ" -> "Aì‚¬")
        if "_" in proposal_name:
            company_name = proposal_name.split("_")[0]
        else:
            company_name = proposal_name
        company_map[proposal_name] = company_name
        print(f"INFO: íšŒì‚¬ ë§¤í•‘ - {proposal_name} -> {company_name}")
    

    # ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ë˜ëŠ” ìƒì„±
    if os.path.exists(CHROMA_PERSIST_DIR):
        print(f"\nê¸°ì¡´ ë²¡í„° DBë¥¼ '{CHROMA_PERSIST_DIR}' ê²½ë¡œì—ì„œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤...")
        embedding_model = initialize_rag_components()
        unified_vectorstore = Chroma(
            persist_directory=CHROMA_PERSIST_DIR,
            embedding_function=embedding_model
        )
        print("âœ“ DB ë¡œë“œ ì™„ë£Œ!")
    else:
        embedding_model = initialize_rag_components()
        unified_vectorstore = create_unified_vectorstore(
            proposal_files, RFP_PATH, embedding_model
        )

    # í‰ê°€ ê¸°ì¤€í‘œë¥¼ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë¡œë“œ (íŒŒì‹± ì—†ìŒ)
    evaluation_criteria_text = load_evaluation_criteria_as_text(EVALUATION_CRITERIA_PATH)
    
    if not evaluation_criteria_text:
        print("ERROR: í‰ê°€ ê¸°ì¤€í‘œë¥¼ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # LLM ì´ˆê¸°í™”
    llm = get_llm_model()

    # =================================================================
    # Phase 1: Dispatcherê°€ ë§ˆí¬ë‹¤ìš´ì„ ë¶„ì„í•˜ì—¬ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§Œ ì¶”ì¶œ
    # =================================================================
    print("\n--- [Phase 1] LLMì´ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ì¹´í…Œê³ ë¦¬)ë§Œ ì¶”ì¶œí•©ë‹ˆë‹¤ ---")
    
    dispatcher_agent = Agent(
        role="í‰ê°€ ê¸°ì¤€í‘œ êµ¬ì¡° ë¶„ì„ ì „ë¬¸ê°€",
        goal="ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ë©”ì¸ ì¹´í…Œê³ ë¦¬)ë§Œ ì¶”ì¶œ",
        backstory="""ë‹¹ì‹ ì€ ë³µì¡í•œ ë¬¸ì„œì˜ ê³„ì¸µ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
        í‰ê°€ ê¸°ì¤€í‘œì—ì„œ ê°€ì¥ í° ë¶„ë¥˜ ì²´ê³„ë§Œì„ ì •í™•íˆ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.""",
        llm=llm,
        verbose=True
    )

    dispatcher_task = Task(
        description=f"""ì•„ë˜ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ í‰ê°€ ê¸°ì¤€í‘œë¥¼ ë¶„ì„í•˜ì—¬ **ìµœìƒìœ„ ëŒ€ë¶„ë¥˜(ë©”ì¸ ì¹´í…Œê³ ë¦¬)**ë§Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

[í‰ê°€ ê¸°ì¤€í‘œ]:
```markdown
{evaluation_criteria_text}
```

ìœ„ í‰ê°€ ê¸°ì¤€í‘œì—ì„œ:
1. ê°€ì¥ í° ë¶„ë¥˜ ì²´ê³„(ìµœìƒìœ„ ëŒ€ë¶„ë¥˜)ë§Œ ì°¾ì•„ì£¼ì„¸ìš”
2. í•˜ìœ„ ì„¸ë¶€ í•­ëª©ë“¤ì€ ë¬´ì‹œí•˜ê³ , í° ì¹´í…Œê³ ë¦¬ì˜ ì´ë¦„ë§Œ ì¶”ì¶œí•˜ì„¸ìš”
3. ì˜ˆë¥¼ ë“¤ì–´ "ê°€ê²© í‰ê°€", "ê¸°ìˆ ë ¥ í‰ê°€", "í”„ë¡œì íŠ¸ ìˆ˜í–‰ ëŠ¥ë ¥" ê°™ì€ ë©”ì¸ ì¹´í…Œê³ ë¦¬ë“¤ì…ë‹ˆë‹¤

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "categories": [
    {{
      "name": "ëŒ€ë¶„ë¥˜1 ì´ë¦„",
      "description": "ì´ ëŒ€ë¶„ë¥˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
    }},
    {{
      "name": "ëŒ€ë¶„ë¥˜2 ì´ë¦„", 
      "description": "ì´ ëŒ€ë¶„ë¥˜ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…"
    }}
  ]
}}

ì£¼ì˜ì‚¬í•­:
- 'ì†Œê³„', 'ì´ê³„', 'í•©ê³„' ê°™ì€ ê²ƒì€ ì œì™¸í•˜ì„¸ìš”
- ì˜¤ì§ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš” (ë³´í†µ 3-5ê°œ ì •ë„)
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜í•˜ì„¸ìš”
""",
        expected_output="ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ì˜ ì´ë¦„ê³¼ ì„¤ëª…ì„ í¬í•¨í•œ JSON ë°°ì—´",
        agent=dispatcher_agent
    )

    dispatcher_crew = Crew(
        agents=[dispatcher_agent], 
        tasks=[dispatcher_task], 
        verbose=False,
        task_callback=partial(task_callback, company="Dispatcher")
    )
    categorization_result = dispatcher_crew.kickoff()
    
    try:
        raw_result = str(categorization_result.raw)
        # JSON ì¶”ì¶œ
        if "```json" in raw_result:
            start_idx = raw_result.find("```json") + 7
            end_idx = raw_result.find("```", start_idx)
            json_string = raw_result[start_idx:end_idx].strip()
        elif "```" in raw_result:
            start_idx = raw_result.find("```") + 3
            end_idx = raw_result.find("```", start_idx)
            json_string = raw_result[start_idx:end_idx].strip()
        else:
            start_idx = raw_result.find('{')
            end_idx = raw_result.rfind('}') + 1
            json_string = raw_result[start_idx:end_idx]
        
        categories_data = json.loads(json_string)
        main_categories = categories_data.get('categories', [])
        
        print(f"[SUCCESS] LLMì´ ì¶”ì¶œí•œ ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ ({len(main_categories)}ê°œ):")
        for cat in main_categories:
            print(f"  - {cat['name']}: {cat.get('description', 'N/A')}")
            
    except (json.JSONDecodeError, ValueError) as e:
        print(f"[ERROR] ëŒ€ë¶„ë¥˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        print(f"   - ì›ë³¸ ê²°ê³¼: {categorization_result.raw}")
        return

    # =================================================================
    # Phase 2: ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë³„ë¡œë§Œ Agent ìƒì„± (1ê°œ ëŒ€ë¶„ë¥˜ = 1ê°œ Agent)
    # =================================================================
    print(f"\n--- [Phase 2] ì¶”ì¶œëœ {len(main_categories)}ê°œ ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ìƒì„±í•©ë‹ˆë‹¤ ---")
    
    # ëª¨ë“  ì œì•ˆì„œ íŒŒì¼ì— ëŒ€í•´ í‰ê°€ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        print(f"\n\n{'='*20} [{proposal_name}] í‰ê°€ ì‹œì‘ {'='*20}")

        specialist_agents = []
        evaluation_tasks = []

        # ê° ìµœìƒìœ„ ëŒ€ë¶„ë¥˜ë§ˆë‹¤ 1ê°œì˜ Agentì™€ 1ê°œì˜ Task ìƒì„±
        for category in main_categories:
            category_name = category['name']
            category_desc = category.get('description', '')
            
            # ëŒ€ë¶„ë¥˜ ì „ë¬¸ê°€ Agent ìƒì„±
            specialist_agent = Agent(
                role=f"'{category_name}' ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€",
                goal=f"'{proposal_name}' ì œì•ˆì„œì˜ '{category_name}' ë¶€ë¬¸ì„ ê°„ê²°í•˜ê²Œ í‰ê°€ (1000ì ì´ë‚´ í•„ìˆ˜)",
                backstory=f"""ë‹¹ì‹ ì€ '{category_name}' ë¶„ì•¼ì˜ ìµœê³  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
                {category_desc}
                
                **ì¤‘ìš”**: ë‹¹ì‹ ì˜ ë³´ê³ ì„œëŠ” ë‹¤ë¥¸ ì‹œìŠ¤í…œì— ì…ë ¥ë˜ë¯€ë¡œ ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.
                ê°„ê²°í•˜ê³  í•µì‹¬ë§Œ ë‹´ì€ í‰ê°€ê°€ ìš”êµ¬ë©ë‹ˆë‹¤.""",
                llm=llm,
                verbose=True
            )
            specialist_agents.append(specialist_agent)
            
            # í•´ë‹¹ ëŒ€ë¶„ë¥˜ ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ RAGì—ì„œ ê²€ìƒ‰ (ê°„ê²°í•˜ê²Œ)
            context = get_context_for_category(
                os.path.splitext(proposal_name)[0],
                f"{category_name} {category_desc}"
            )
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½ (í† í° ì ˆì•½)
            if len(context) > 2000:
                context = context[:2000] + "\n...(ì´í•˜ ìƒëµ)"
                print(f"  âš ï¸ '{category_name}' ì»¨í…ìŠ¤íŠ¸ë¥¼ 2000ìë¡œ ì œí•œ")
            
            # ëŒ€ë¶„ë¥˜ ì „ì²´ë¥¼ í‰ê°€í•˜ëŠ” ë‹¨ì¼ Task ìƒì„±
            task = Task(
                description=f"""ì œì•ˆì„œ '{proposal_name}'ì˜ '{category_name}' ë¶€ë¬¸ì„ í‰ê°€í•˜ì„¸ìš”.

**í‰ê°€ ëŒ€ë¶„ë¥˜**: {category_name}

**ì œì•ˆì„œ í•µì‹¬ ë‚´ìš©**:
{context}

**ğŸš¨ğŸš¨ğŸš¨ ì ˆëŒ€ ê·œì¹™: ì „ì²´ ì‘ë‹µì€ ë°˜ë“œì‹œ 1000ì ì´ë‚´ë¡œ ì‘ì„±í•˜ì„¸ìš”! ğŸš¨ğŸš¨ğŸš¨**

ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ **ê·¹ë„ë¡œ ê°„ê²°í•˜ê²Œ** ì‘ì„±:

# {category_name} ë¶€ë¬¸

**ë°°ì /ì·¨ë“**: Yì  / Zì  (X%)

**ì£¼ìš” í•­ëª©** (2-3ê°œë§Œ):
- í•­ëª©1 (ë°°ì ): ì ìˆ˜ - 1ì¤„ í‰ê°€
- í•­ëª©2 (ë°°ì ): ì ìˆ˜ - 1ì¤„ í‰ê°€

**ê°•ì **: (2ê°œ, ê° 1ì¤„)
- 
- 

**ì•½ì **: (2ê°œ, ê° 1ì¤„)
- 
- 

**ê°œì„ ì•ˆ**: (1ê°œ, 1ì¤„)
- 

**ì ˆëŒ€ ê·œì¹™**:
- 1000ì ì´ˆê³¼ ì‹œ ì‘ë‹µ ë¬´íš¨
- ë¶ˆí•„ìš”í•œ ë¬¸ì¥ ì¼ì²´ ê¸ˆì§€
- í•­ëª©ì€ ìµœëŒ€ 3ê°œë§Œ
- ê° ì„¤ëª…ì€ 1ì¤„ë§Œ
""",
                expected_output=f"'{category_name}' ë¶€ë¬¸ í‰ê°€ (1000ì ì´ë‚´ í•„ìˆ˜)",
                agent=specialist_agent
            )
            evaluation_tasks.append(task)
        
        if not evaluation_tasks:
            print("í‰ê°€í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        print(f"\nì´ {len(specialist_agents)}ê°œì˜ ì „ë¬¸ê°€ Agentê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì´ {len(evaluation_tasks)}ê°œì˜ í‰ê°€ Taskê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

        # í˜„ì¬ íšŒì‚¬ëª…ìœ¼ë¡œ task_callback ìƒì„±
        current_company = company_map.get(proposal_name, "Unknown")
        current_task_callback = partial(task_callback, company=current_company)
        
        evaluation_crew = Crew(
            agents=specialist_agents,
            tasks=evaluation_tasks,
            verbose=False,
            task_callback=current_task_callback
        )
        final_results = await evaluation_crew.kickoff_async()

        print(f"\n--- [Phase 3] [{proposal_name}] ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤ ---")
        
        # kickoff_async()ëŠ” íŠœí”Œì„ ë°˜í™˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì²˜ë¦¬
        if isinstance(final_results, tuple):
            results_list = final_results[0] if final_results else []
        else:
            results_list = final_results
        
        # ë¶€ë¬¸ë³„ ë³´ê³ ì„œë¥¼ ìˆ˜ì§‘í•˜ê³  ê°•ì œë¡œ ê¸¸ì´ ì œí•œ
        individual_reports = []
        for idx, result in enumerate(results_list):
            if hasattr(result, 'raw'):
                report_text = str(result.raw)
            else:
                report_text = str(result)
            
            # ê°•ì œë¡œ 1000ì ì œí•œ (LLMì´ ë¬´ì‹œí•œ ê²½ìš° ëŒ€ë¹„)
            if len(report_text) > 1000:
                report_text = report_text[:1000] + "\n...(ê¸¸ì´ ì œí•œìœ¼ë¡œ ì ˆì‚­)"
                print(f"  âš ï¸ ë¶€ë¬¸ {idx+1} ë³´ê³ ì„œê°€ 1000ìë¥¼ ì´ˆê³¼í•˜ì—¬ ê°•ì œ ì ˆì‚­")
            
            individual_reports.append(report_text)
        
        individual_reports_text = "\n\n".join(individual_reports)
        
        # í† í° ìˆ˜ í™•ì¸ì„ ìœ„í•œ ì¶œë ¥
        print(f"INFO: ë¶€ë¬¸ë³„ ë³´ê³ ì„œ ì´ ê¸¸ì´: {len(individual_reports_text)}ì")
        
        # ê·¸ë˜ë„ ë„ˆë¬´ ê¸¸ë©´ ì¶”ê°€ ê²½ê³ 
        if len(individual_reports_text) > 4000:
            print(f"  âš ï¸âš ï¸ ê²½ê³ : ë¶€ë¬¸ë³„ ë³´ê³ ì„œ ì´í•©ì´ {len(individual_reports_text)}ìë¡œ ì—¬ì „íˆ ê¹ë‹ˆë‹¤!")
            print(f"  â†’ ê° ë¶€ë¬¸ì„ 500ìë¡œ ì¶”ê°€ ì œí•œí•©ë‹ˆë‹¤.")
            individual_reports = [report[:500] + "..." for report in individual_reports]
        individual_reports_text = "\n\n".join(individual_reports)
        print(f"  â†’ ìµœì¢… ê¸¸ì´: {len(individual_reports_text)}ì")

        reporting_agent = Agent(
            role="ìˆ˜ì„ í‰ê°€ ë¶„ì„ê°€",
            goal="ë¶€ë¬¸ë³„ í‰ê°€ë¥¼ ì¢…í•©í•˜ì—¬ ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ì™„ì„±ëœ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±",
            backstory="""ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ ìˆ˜ì„ ë¶„ì„ê°€ë¡œ, í•µì‹¬ì„ íŒŒì•…í•˜ê³  ì „ëµì  ì¸ì‚¬ì´íŠ¸ë¥¼ 
            ì œê³µí•˜ëŠ” ëŠ¥ë ¥ì´ ë›°ì–´ë‚˜ë©°, ì˜ì‚¬ê²°ì •ìë“¤ì´ ì‹ ë¢°í•˜ëŠ” ë¶„ì„ê°€ì…ë‹ˆë‹¤.""",
            llm=get_llm_model(), 
            verbose=True
        )

        reporting_task = Task(
            description=f"""'{proposal_name}' ì œì•ˆì„œì— ëŒ€í•œ ë¶€ë¬¸ë³„ í‰ê°€ë¥¼ ì¢…í•©í•˜ì—¬ 
ìµœì¢… í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ë¶€ë¬¸ë³„ í‰ê°€ ë³´ê³ ì„œë“¤**:
{individual_reports_text}

**ğŸš¨ ì¤‘ìš”: ìµœì¢… ë³´ê³ ì„œëŠ” 2000ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”!**

ë‹¤ìŒ êµ¬ì¡°ë¡œ ì‘ì„±í•˜ì„¸ìš”:

# ì œì•ˆì„œ í‰ê°€ ìµœì¢… ë³´ê³ ì„œ

## 1. Executive Summary (5ì¤„ ì´ë‚´)
- ì œì•ˆì„œ: {proposal_name}
- í‰ê°€ì¼: {datetime.now().strftime('%Y-%m-%d')}
- ì´ì : X/100ì 
- ë“±ê¸‰: S/A/B/C/D
- í•µì‹¬ í‰ê°€: (2ì¤„ ìš”ì•½)

## 2. ë¶€ë¬¸ë³„ ê²°ê³¼ (í‘œ í˜•ì‹)
| ë¶€ë¬¸ | ë°°ì  | ì·¨ë“ | ë¹„ìœ¨ | í‰ê°€ |
|------|------|------|------|------|
| ë¶€ë¬¸1 | XX | YY | ZZ% | 1ì¤„ í‰ê°€ |
| ... |
| ì´ì  | 100 | ì´í•© | í‰ê·  | - |

## 3. ì£¼ìš” ë°œê²¬ì‚¬í•­
**ê°•ì  Top 3** (ê° 1ì¤„):
1. 
2. 
3. 

**ì•½ì  Top 3** (ê° 1ì¤„):
1. 
2. 
3. 

## 4. ê¶Œê³ ì‚¬í•­
**ìš°ì„  ê°œì„ ** (2ê°œ, ê° 1ì¤„):
- 
- 

**ê°•ì  ìœ ì§€** (2ê°œ, ê° 1ì¤„):
- 
- 

## 5. ìµœì¢… ê²°ë¡  (3ì¤„ ì´ë‚´)
- ì¢…í•© ì˜ê²¬:
- ì„ ì • ê¶Œê³ : (ì„ ì • ì¶”ì²œ/ì¡°ê±´ë¶€/íƒˆë½)
- ê·¼ê±°:

**ì‘ì„± ê·œì¹™**:
- ì „ì²´ 2000ì ì´ë‚´ ì—„ìˆ˜
- í‘œì™€ ë¦¬ìŠ¤íŠ¸ í™œìš©í•˜ì—¬ ê°„ê²°í•˜ê²Œ
- ì¤‘ë³µ ë‚´ìš© ì œê±°
- í•µì‹¬ë§Œ í¬í•¨
""",
            expected_output="ê²½ì˜ì§„ ì˜ì‚¬ê²°ì •ìš© ê°„ê²°í•œ ìµœì¢… í‰ê°€ ë³´ê³ ì„œ (2000ì ì´ë‚´)",
            agent=reporting_agent
        )
        
        # í˜„ì¬ íšŒì‚¬ëª…ìœ¼ë¡œ task_callback ìƒì„±
        current_company = company_map.get(proposal_name, "Unknown")
        current_task_callback = partial(task_callback, company=current_company)
        
        reporting_crew = Crew(
            agents=[reporting_agent], 
            tasks=[reporting_task], 
            verbose=False,
            task_callback=current_task_callback
        )
        final_comprehensive_report = reporting_crew.kickoff()

        print(f"\n\n[FINAL REPORT] [{proposal_name}] ìµœì¢… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ")
        print("="*80)
        print(final_comprehensive_report.raw)
        print("="*80)
        
        # í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        save_evaluation_report(proposal_name, final_comprehensive_report.raw)


# =================================================================
# 6. ê¸°ì—…ë³„ Trace ì €ì¥ í•¨ìˆ˜
# =================================================================

def save_task_trace(company: str, task_info: dict):
    """ê¸°ì—…ë³„ë¡œ task traceë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
    trace_dir = os.path.join("traces", company)
    os.makedirs(trace_dir, exist_ok=True)
    filepath = os.path.join(trace_dir, "task_log.ndjson")
    with open(filepath, "a", encoding="utf-8") as f:
        f.write(json.dumps(task_info, ensure_ascii=False) + "\n")

def task_callback(task_output, company="Unknown"):
    """ê° ì‘ì—… ì™„ë£Œ ì‹œ ê²°ê³¼ë¥¼ ë¡œê¹…í•˜ëŠ” ì½œë°± (ê¸°ì—…ë³„ ì €ì¥)"""
    global company_map
    
    task_info = {
        "type": "task_completed",
        "task_name": getattr(task_output, "name", None),
        "agent": str(getattr(task_output, "agent", None)),
        "status": "completed",
        "timestamp": datetime.utcnow().isoformat() + "Z",
    }
    
    # ê¸°ì—…ë³„ trace ì €ì¥
    save_task_trace(company, task_info)

# =================================================================
# 7. ì±—ë´‡ ê´€ë ¨ í•¨ìˆ˜ë“¤ (main_jy.pyì—ì„œ ê°€ì ¸ì˜´)
# =================================================================
def normalize_company_name(extracted: str) -> str:   ### ğŸ”§ FIX: ì¤‘ë³µ ì •ì˜ ì œê±° í›„ ìµœì¢… ë²„ì „
    """ì¶”ì¶œëœ íšŒì‚¬ëª…ì„ proposal_idë¡œ ë³€í™˜, ì—†ìœ¼ë©´ all"""
    if not extracted or extracted=="all":
        return "all"
    extracted_clean = extracted.replace(" ","")
    for pid, cname in company_map.items():
        if extracted_clean in cname.replace(" ",""):
            return pid
    return "all"


def classify_question(user_question: str) -> dict:
    router_agent = Agent(
        role="ì§ˆë¬¸ ë¶„ë¥˜ ì „ë¬¸ê°€",
        goal="ì‚¬ìš©ì ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ë¥˜í•˜ì—¬ intent (íšŒì‚¬ ë‚´ë¶€ ì •ë³´, í‰ê°€ ê´€ë ¨, ì¼ë°˜ ì§ˆë¬¸) ê²°ì •, íšŒì‚¬ëª… ì¶”ì¶œì¶œ",
        backstory="ë‹¹ì‹ ì€ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. íšŒì‚¬ ë‚´ë¶€ ì •ë³´, í‰ê°€ ê´€ë ¨, ì¼ë°˜ ì§ˆë¬¸ì„ ì •í™•íˆ êµ¬ë¶„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        llm=get_llm_model(),
        verbose=True
    )
    router_task = Task(
        description=f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”:

ì§ˆë¬¸: "{user_question}"

ë¶„ë¥˜ ê¸°ì¤€:
1. "company_db": íšŒì‚¬ ë‚´ë¶€ ì •ë³´ (ë¶€ì„œ, ê¸°ìˆ ìŠ¤íƒ, ë‹´ë‹¹ì, ì¡°ì§ë„ ë“±)
2. "evaluation": ì œì•ˆì„œ í‰ê°€ ê´€ë ¨ (ì ìˆ˜, ê·¼ê±°, ë³´ê³ ì„œ, í‰ê°€ ê²°ê³¼ ë“±)
3. "other": ìœ„ ë‘ ê°€ì§€ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ì¼ë°˜ ì§ˆë¬¸

íšŒì‚¬ëª… ì¶”ì¶œ:
- í‰ê°€ ê´€ë ¨ ì§ˆë¬¸ì—ì„œ íšŒì‚¬ëª…ì´ ì–¸ê¸‰ë˜ë©´ ì¶”ì¶œ
- ì—†ìœ¼ë©´ "all"

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
  "intent": "company_db",
  "company": "Aì‚¬"
}}
        """,
        expected_output="JSON ê°ì²´ (intentì™€ company í‚¤ í¬í•¨)",
        agent=router_agent
    )
    crew = Crew(agents=[router_agent],tasks=[router_task],verbose=False)
    result = crew.kickoff()
    try:
        # JSON ì¶”ì¶œ ë° íŒŒì‹±
        raw_text = str(result.raw)
        if "```json" in raw_text:
            start = raw_text.find("```json") + 7
            end = raw_text.find("```", start)
            json_text = raw_text[start:end].strip()
        elif "```" in raw_text:
            start = raw_text.find("```") + 3
            end = raw_text.find("```", start)
            json_text = raw_text[start:end].strip()
        else:
            start = raw_text.find('{')
            end = raw_text.rfind('}') + 1
            json_text = raw_text[start:end]
        
        parsed = json.loads(json_text)
        return parsed
    except Exception as e:
        print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        return {"intent":"other","company":"all"}


def search_company_db(user_question: str) -> str:
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì‚¬ë‚´ ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì—¬ ë‹µë³€í•©ë‹ˆë‹¤."""
    global unified_vectorstore

    if unified_vectorstore is None:
        return "ì˜¤ë¥˜: ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."

    # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ì‚¬ë‚´ ì •ë³´ ê²€ìƒ‰ (ì‚¬ë‚´_ë¡œ ì‹œì‘í•˜ëŠ” proposal_id í•„í„°ë§)
    try:
        # ëª¨ë“  ì‚¬ë‚´ ì •ë³´ íƒ€ì…ì—ì„œ ê²€ìƒ‰
        all_results = []
        internal_types = ["ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ", "ì‚¬ë‚´_ë‹´ë‹¹ì", "ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜", "ì‚¬ë‚´_ì¥ì• ì´ë ¥", "ì‚¬ë‚´_ê¸°íƒ€"]

        for doc_type in internal_types:
            results = unified_vectorstore.similarity_search(
                query=user_question,
                k=3,
                filter={"proposal_id": f"internal_{doc_type}"}
            )
            all_results.extend(results)

        if not all_results:
            return "ê´€ë ¨ëœ ì‚¬ë‚´ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        context = "\n\n".join([doc.page_content for doc in all_results[:5]])  # ìƒìœ„ 5ê°œë§Œ ì‚¬ìš©

        prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_question}"

íšŒì‚¬ ë‚´ë¶€ ì •ë³´:
{context}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
- êµ¬ì²´ì ì¸ ì •ë³´ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”
"""
        return get_llm_model().call(prompt)

    except Exception as e:
        return f"ì‚¬ë‚´ ì •ë³´ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}"

# ================================================================
# Evaluation ì§ˆë¬¸ ì²˜ë¦¬
# ================================================================
def load_evaluation_trace(company: str):
    filepath = os.path.join("traces", company, "task_log.ndjson")
    if not os.path.exists(filepath):
        return []
    with open(filepath, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]

def answer_evaluation_question(user_question: str, company="all") -> str:
    if company == "all":
        companies = list(company_map.keys())
    else:
        companies = [company]
    
    context = ""
    for comp in companies:
        traces = load_evaluation_trace(comp)
        for t in traces[:5]:
            task_name = t.get('task_name', 'Unknown')
            status = t.get('status', 'Unknown')
            context += f"- {task_name}: {status}\n"
    
    if not context:
        return f"í˜„ì¬ {company} íšŒì‚¬ì˜ í‰ê°€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì œì•ˆì„œ í‰ê°€ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”."
    
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_question}"

í‰ê°€ ê¸°ë¡ ë°ì´í„°:
{context}

ìœ„ í‰ê°€ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
- ì‹¤ì œ í‰ê°€ ê²°ê³¼ë¥¼ ì¸ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
- êµ¬ì²´ì ì¸ ì ìˆ˜ë‚˜ ê·¼ê±°ê°€ ìˆë‹¤ë©´ ëª…ì‹œí•˜ì„¸ìš”
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
"""
    return get_llm_model().call(prompt)

def answer_general_question(user_question: str) -> str:
    prompt = f"""
ì‚¬ìš©ì ì§ˆë¬¸: "{user_question}"

ìœ„ ì§ˆë¬¸ì— ëŒ€í•´ ì •í™•í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ë‹µë³€ì„ í•´ì£¼ì„¸ìš”.
- ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”
- ê´€ë ¨ ì—†ëŠ” ì •ë³´ëŠ” ì œì™¸í•˜ì„¸ìš”
- ì •í™•í•œ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”
"""
    return get_llm_model().call(prompt)


def run_main():
    """ë™ê¸°ì ìœ¼ë¡œ main í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    asyncio.run(main())

def run_chatbot_test():
    print("================================")
    print("ChatBot í…ŒìŠ¤íŠ¸")
    print("================================")
    test_questions = [
        "ìš°ë¦¬ íšŒì‚¬ì—ì„œ Kafka ì“°ëŠ” ë¶€ì„œê°€ ìˆì–´?",
        "ì™œ Aì‚¬ì˜ ê¸°ìˆ  ì ìˆ˜ëŠ” 8ì ì´ì•¼?", 
        "ì˜¤ëŠ˜ ë‚ ì”¨ ì–´ë•Œ?",
        "ê°œë°œíŒ€ì—ì„œ ë­ ì“°ê³  ìˆì–´?",
        "React ì‚¬ìš©í•˜ëŠ” íŒ€ì´ ì–´ë””ì•¼?",
        "Aì‚¬ì˜ ê¸°ìˆ  ì ìˆ˜ ê·¼ê±°ë¥¼ ì•Œë ¤ì¤˜",
    ]
    
    for q in test_questions:
        result = classify_question(q)
        intent, company = result.get("intent"), result.get("company", "all")
        print(f"\nQ: {q}\në¶„ë¥˜: {result}")
        if intent == "company_db":
            print("A:", search_company_db(q))
        elif intent == "evaluation":
            comp_id = normalize_company_name(company)
            print("A:", answer_evaluation_question(q, comp_id))
        else:
            print("A:", answer_general_question(q))

if __name__ == '__main__':
    run_main()
    run_chatbot_test()