# main_rag.py

import os
import asyncio
import json
import glob
import torch
import chromadb
import re
from datetime import datetime
from dotenv import load_dotenv
from crewai import Agent, Task, Crew
from crewai.llm import LLM

# LangChain ê´€ë ¨ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document  # ì‚¬ë‚´ ì •ë³´ Document ìƒì„±ìš©

load_dotenv()

# =================================================================
# 1. RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ë° í•¨ìˆ˜ ì •ì˜
# =================================================================

# --- ê²½ë¡œ ì„¤ì • ---
PROPOSAL_DIR = "./proposal"
RFP_PATH = "./RFP/ìˆ˜í˜‘_rfp.txt"
OUTPUT_DIR = "./output"
EVALUATION_CRITERIA_PATH = "./standard/evaluation_criteria.md"
INTERNAL_DATA_DIR = "./internal_data"  # ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ (ê¸°ìˆ ìŠ¤íƒ, ë‹´ë‹¹ì, ë§ˆì´ê·¸ë ˆì´ì…˜, ì¥ì• ì´ë ¥ ë“±)

# --- ì „ì—­ ë³€ìˆ˜ ---
# ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì €ì¥í•  ì „ì—­ ë³€ìˆ˜
# Agentë“¤ì´ ê³µìœ í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
unified_vectorstore = None

def initialize_rag_components():
    """RAGì— í•„ìš”í•œ ì„ë² ë”© ëª¨ë¸ê³¼ ChromaDB í´ë¼ì´ì–¸íŠ¸ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    # CUDA ê°•ì œ ì‚¬ìš© ì„¤ì •
    if torch.cuda.is_available():
        device = "cuda"
        torch.cuda.set_device(0)  # ì²« ë²ˆì§¸ GPU ì‚¬ìš©
        print(f"INFO: CUDA ì‚¬ìš© ê°€ëŠ¥ - GPU: {torch.cuda.get_device_name(0)}")
        print(f"INFO: GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
    else:
        device = "cpu"
        print("WARNING: CUDAë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
    
    print("INFO: ì„ë² ë”© ëª¨ë¸ì„ ë¡œë”©í•©ë‹ˆë‹¤...")
    embedding_model = HuggingFaceEmbeddings(
        model_name="Qwen/Qwen3-Embedding-0.6B",
        model_kwargs={'device': device},
        encode_kwargs={'normalize_embeddings': True}
    )
    
    chroma_client = chromadb.PersistentClient(path="./chroma_db_crewai")
    print("INFO: ì„ë² ë”© ëª¨ë¸ ë° ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ.")
    return embedding_model, chroma_client

def load_document(file_path, doc_type, proposal_name):
    """ë¬¸ì„œ ë¡œë“œ (TXT ì§€ì›)"""
    documents = []
    print(f"  - [{doc_type}] '{os.path.basename(file_path)}' ë¡œë”© ì¤‘...")
    
    if file_path.endswith('.txt'):
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()
        # ê° Documentì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
        for doc in docs:
            doc.metadata.update({"doc_type": doc_type, "proposal_name": proposal_name})
        documents.extend(docs)

    # HTML íŒŒì¼ ë¡œë“œ (ì¶”ê°€)
    elif file_path.endswith('.html'):
        loader = TextLoader(file_path, encoding='utf-8')
        docs = loader.load()
        for doc in docs:
            doc.metadata.update({"doc_type": doc_type, "proposal_name": proposal_name})
        documents.extend(docs)
    
    print(f"    â†’ {len(documents)}ê°œ ì„¹ì…˜ ë¡œë“œë¨")
    return documents


# =================================================================
# ì‚¬ë‚´ ì •ë³´ ë¡œë” í•¨ìˆ˜ë“¤
# =================================================================

def load_internal_structured_data(file_path, doc_type_prefix):
    """
    ì •í˜•í™”ëœ ì‚¬ë‚´ ì •ë³´ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    ë°ì´í„° í˜•ì‹: '---'ë¡œ êµ¬ë¶„ëœ í•­ëª©ë“¤
    ê° í•­ëª©ì€ key: value í˜•ì‹ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨

    Args:
        file_path (str): ì‚¬ë‚´ ì •ë³´ íŒŒì¼ ê²½ë¡œ
        doc_type_prefix (str): ë¬¸ì„œ íƒ€ì… ì ‘ë‘ì‚¬ (ì˜ˆ: "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ")

    Returns:
        list[Document]: ë¡œë“œëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸
    """
    documents = []

    # íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•Šìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    if not os.path.exists(file_path):
        print(f"  âš  íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {file_path}")
        return documents

    print(f"  - [{doc_type_prefix}] '{os.path.basename(file_path)}' ë¡œë”© ì¤‘...")

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # '---'ë¡œ êµ¬ë¶„ëœ ê° í•­ëª©ì„ ê°œë³„ Documentë¡œ ë³€í™˜
    entries = content.split('---')

    for entry in entries:
        entry = entry.strip()
        if not entry:  # ë¹ˆ í•­ëª©ì€ ìŠ¤í‚µ
            continue

        # ê¸°ë³¸ ë©”íƒ€ë°ì´í„° ì„¤ì •
        metadata = {"doc_type": doc_type_prefix}

        # ê° ì¤„ì„ íŒŒì‹±í•˜ì—¬ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
        # í˜•ì‹: key: value
        lines = entry.split('\n')
        for line in lines:
            if ':' in line:
                key, value = line.split(':', 1)
                # ë©”íƒ€ë°ì´í„°ì— í‚¤-ê°’ ì €ì¥ (ê³µë°± ì œê±°)
                metadata[key.strip()] = value.strip()

        # Document ìƒì„± (ì „ì²´ ë‚´ìš©ì„ page_contentë¡œ, íŒŒì‹±ëœ ì •ë³´ëŠ” metadataë¡œ)
        doc = Document(
            page_content=entry,  # ì›ë³¸ í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ì €ì¥
            metadata=metadata
        )
        documents.append(doc)

    print(f"    â†’ {len(documents)}ê°œ í•­ëª© ë¡œë“œë¨")
    return documents


def load_all_internal_data(internal_data_dir):
    """
    ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ìë™ìœ¼ë¡œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    íŒŒì¼ëª… ê·œì¹™:
    - tech_stacks*.txt â†’ ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ
    - contacts*.txt â†’ ì‚¬ë‚´_ë‹´ë‹¹ì
    - migrations*.txt â†’ ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜
    - incidents*.txt â†’ ì‚¬ë‚´_ì¥ì• ì´ë ¥

    Args:
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        list[Document]: ëª¨ë“  ì‚¬ë‚´ ì •ë³´ Document ë¦¬ìŠ¤íŠ¸
    """
    all_internal_docs = []

    if not os.path.exists(internal_data_dir):
        print(f"  âš  ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {internal_data_dir}")
        return all_internal_docs

    print(f"\n[ì‚¬ë‚´ ì •ë³´ ë¡œë“œ ì‹œì‘: {internal_data_dir}]")

    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .txt íŒŒì¼ ê²€ìƒ‰
    internal_files = glob.glob(os.path.join(internal_data_dir, "*.txt"))

    for file_path in internal_files:
        filename = os.path.basename(file_path).lower()

        # íŒŒì¼ëª…ì— ë”°ë¼ ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
        if 'tech_stack' in filename or 'technology' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ")
            all_internal_docs.extend(docs)

        elif 'contact' in filename or 'person' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ë‹´ë‹¹ì")
            all_internal_docs.extend(docs)

        elif 'migration' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜")
            all_internal_docs.extend(docs)

        elif 'incident' in filename or 'failure' in filename:
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ì¥ì• ì´ë ¥")
            all_internal_docs.extend(docs)

        else:
            # ë¶„ë¥˜ë˜ì§€ ì•Šì€ íŒŒì¼ì€ ì¼ë°˜ ì‚¬ë‚´ ì •ë³´ë¡œ ì²˜ë¦¬
            docs = load_internal_structured_data(file_path, "ì‚¬ë‚´_ê¸°íƒ€")
            all_internal_docs.extend(docs)

    print(f"âœ… ì´ {len(all_internal_docs)}ê°œì˜ ì‚¬ë‚´ ì •ë³´ í•­ëª© ë¡œë“œ ì™„ë£Œ\n")
    return all_internal_docs


def load_all_internal_data_simple(internal_data_dir):
    """
    ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  íŒŒì¼ì„ ê°„ë‹¨í•˜ê²Œ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜

    ì •í˜•/ë¹„ì •í˜• êµ¬ë¶„ ì—†ì´ ëª¨ë“  .txt íŒŒì¼ì„ Documentë¡œ ë³€í™˜í•˜ì—¬ ë¡œë“œí•©ë‹ˆë‹¤.
    ê° íŒŒì¼ì€ í†µì§¸ë¡œ í•˜ë‚˜ì˜ Documentê°€ ë˜ë©°, íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì…ì„ ìë™ ì¶”ë¡ í•©ë‹ˆë‹¤.

    Args:
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ

    Returns:
        list[Document]: ëª¨ë“  ì‚¬ë‚´ ì •ë³´ Document ë¦¬ìŠ¤íŠ¸
    """
    all_internal_docs = []

    if not os.path.exists(internal_data_dir):
        print(f"  âš  ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {internal_data_dir}")
        return all_internal_docs

    print(f"\n[ì‚¬ë‚´ ì •ë³´ ë¡œë“œ ì‹œì‘: {internal_data_dir}]")

    # ë””ë ‰í† ë¦¬ ë‚´ ëª¨ë“  .txt íŒŒì¼ ê²€ìƒ‰
    internal_files = glob.glob(os.path.join(internal_data_dir, "*.txt"))

    for file_path in internal_files:
        filename = os.path.basename(file_path)

        try:
            # íŒŒì¼ ë‚´ìš© ì „ì²´ë¥¼ ì½ê¸°
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì… ìë™ ë¶„ë¥˜
            if 'tech_stack' in filename.lower():
                doc_type = "ì‚¬ë‚´_ê¸°ìˆ ìŠ¤íƒ"
            elif 'contact' in filename.lower():
                doc_type = "ì‚¬ë‚´_ë‹´ë‹¹ì"
            elif 'migration' in filename.lower():
                doc_type = "ì‚¬ë‚´_ë§ˆì´ê·¸ë ˆì´ì…˜"
            elif 'incident' in filename.lower():
                doc_type = "ì‚¬ë‚´_ì¥ì• ì´ë ¥"
            else:
                doc_type = "ì‚¬ë‚´_ê¸°íƒ€"

            # Document ìƒì„± (íŒŒì¼ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ Documentë¡œ)
            doc = Document(
                page_content=content,
                metadata={
                    "doc_type": doc_type,
                    "source_file": filename
                }
            )
            all_internal_docs.append(doc)
            print(f"  âœ“ [{doc_type}] {filename} ë¡œë“œ ì™„ë£Œ ({len(content)} ì)")

        except Exception as e:
            print(f"  âœ— {filename} ë¡œë“œ ì‹¤íŒ¨: {e}")
            continue

    print(f"\nâœ… ì´ {len(all_internal_docs)}ê°œì˜ ì‚¬ë‚´ ì •ë³´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ\n")
    return all_internal_docs


def create_unified_vectorstore(
    proposal_files,
    rfp_path,
    internal_data_dir,
    embedding_model,
    chroma_client,
    collection_name="proposal_evaluation_store"
):
    """
    ì œì•ˆì„œ, RFP, ì‚¬ë‚´ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ëŠ” í†µí•© ë²¡í„°ìŠ¤í† ì–´ë¥¼ ìƒì„±í•˜ëŠ” í•¨ìˆ˜

    Args:
        proposal_files (list): ì œì•ˆì„œ íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
        rfp_path (str): RFP íŒŒì¼ ê²½ë¡œ
        internal_data_dir (str): ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        embedding_model: HuggingFace ì„ë² ë”© ëª¨ë¸
        chroma_client: ChromaDB í´ë¼ì´ì–¸íŠ¸
        collection_name (str): ë²¡í„°ìŠ¤í† ì–´ ì»¬ë ‰ì…˜ ì´ë¦„

    Returns:
        Chroma: ìƒì„±ëœ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
    """
    print(f"\n{'='*70}")
    print(f"  í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì‹œì‘ (Collection: {collection_name})")
    print(f"{'='*70}")

    all_documents = []

    # 1. RFP ë¬¸ì„œ ë¡œë“œ
    print("\n[1ë‹¨ê³„] RFP ë¬¸ì„œ ë¡œë“œ")
    if os.path.exists(rfp_path):
        all_documents.extend(load_document(rfp_path, "RFP", "RFP"))
    else:
        print(f"  âš  RFP íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {rfp_path}")

    # 2. ì œì•ˆì„œ ë¬¸ì„œ ë¡œë“œ
    print("\n[2ë‹¨ê³„] ì œì•ˆì„œ ë¬¸ì„œ ë¡œë“œ")
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        all_documents.extend(load_document(proposal_path, "ì œì•ˆì„œ", proposal_name))

    # 3. ì‚¬ë‚´ ì •ë³´ ë¡œë“œ (ê¸°ìˆ ìŠ¤íƒ, ë‹´ë‹¹ì, ë§ˆì´ê·¸ë ˆì´ì…˜ ì´ë ¥, ì¥ì•  ì´ë ¥ ë“±)
    # ëª¨ë“  íŒŒì¼ì„ ê°„ë‹¨í•˜ê²Œ ë¡œë“œ (ì •í˜•/ë¹„ì •í˜• êµ¬ë¶„ ì—†ì´)
    print("\n[3ë‹¨ê³„] ì‚¬ë‚´ ì •ë³´ ë¡œë“œ")
    internal_docs = load_all_internal_data_simple(internal_data_dir)
    all_documents.extend(internal_docs)  # ì œì•ˆì„œ+RFPì— ì‚¬ë‚´ ì •ë³´ ë¬¸ì„œ ì¶”ê°€

    print(f"\n  [OK] ì´ {len(all_documents)}ê°œ ë¬¸ì„œ ì„¹ì…˜ ë¡œë“œ ì™„ë£Œ")

    # 4. í…ìŠ¤íŠ¸ ë¶„í•  (ì²­í¬ ë‹¨ìœ„ë¡œ ìª¼ê°œê¸°)
    print("\n[4ë‹¨ê³„] í…ìŠ¤íŠ¸ ì²­í¬ ë¶„í• ")
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=30)
    splits = text_splitter.split_documents(all_documents)
    print(f"  [OK] {len(splits)}ê°œ ì²­í¬ë¡œ ë¶„í•  ì™„ë£Œ")

    # 5. ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ìˆì„ ê²½ìš°)
    print("\n[5ë‹¨ê³„] ê¸°ì¡´ ì»¬ë ‰ì…˜ í™•ì¸ ë° ì‚­ì œ")
    try:
        chroma_client.delete_collection(name=collection_name)
        print(f"  [OK] ê¸°ì¡´ '{collection_name}' ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
    except Exception:
        print(f"  â„¹ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ (ì‹ ê·œ ìƒì„±)")

    # 6. ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì„ë² ë”© + ì¸ë±ì‹±)
    print("\n[6ë‹¨ê³„] ë²¡í„° ì„ë² ë”© ë° ì¸ë±ì‹±")
    print(f"  â³ {len(splits)}ê°œ ì²­í¬ë¥¼ ì„ë² ë”© ì¤‘... (ìˆ˜ ë¶„ ì†Œìš”)")
    vectorstore = Chroma.from_documents(
        documents=splits,              # ì²­í¬ ë¶„í• ëœ Document ë¦¬ìŠ¤íŠ¸
        embedding=embedding_model,     # ì„ë² ë”© ëª¨ë¸
        collection_name=collection_name,  # ì»¬ë ‰ì…˜ ì´ë¦„
        client=chroma_client          # ChromaDB í´ë¼ì´ì–¸íŠ¸
    )
    print("  [OK] í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ!")
    return vectorstore

def get_context_for_topic(proposal_file, topic):
    """ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    global unified_vectorstore

    # ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì€ ê²½ìš° ì—ëŸ¬ ë©”ì‹œì§€ ë°˜í™˜
    if unified_vectorstore is None:
        return "ì˜¤ë¥˜: ë²¡í„°ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

    print(f"  ğŸ” RAG ê²€ìƒ‰ ì‹¤í–‰ -> ì œì•ˆì„œ: '{proposal_file}', í† í”½: '{topic}'")

    # ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰ ìˆ˜í–‰
    # - query: ê²€ìƒ‰ ì¿¼ë¦¬ (í† í”½)
    # - k: ë°˜í™˜í•  ê²°ê³¼ ê°œìˆ˜ (ìƒìœ„ 2ê°œ)
    # - filter: ë©”íƒ€ë°ì´í„° í•„í„°ë§ (íŠ¹ì • ì œì•ˆì„œë§Œ ê²€ìƒ‰)
    results = unified_vectorstore.similarity_search(
        query=topic,
        k=2,  # ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ìœ„í•´ 2ê°œë¡œ ì¦ê°€
        filter={"proposal_name": proposal_file}
    )

    # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
    if not results:
        return "ê´€ë ¨ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # ê²€ìƒ‰ëœ ì—¬ëŸ¬ ì²­í¬ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    context = "\n\n---\n\n".join([doc.page_content for doc in results])
    
    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
    if len(context) > 3000:
        context = context[:3000] + "..."
        print(f"INFO: ì»¨í…ìŠ¤íŠ¸ê°€ ê¸¸ì–´ì„œ 3000ìë¡œ ì œí•œí–ˆìŠµë‹ˆë‹¤.")
    
    return context

def load_evaluation_criteria(criteria_path):
    """í‰ê°€ ê¸°ì¤€í‘œë¥¼ ë™ì ìœ¼ë¡œ ë¡œë“œí•©ë‹ˆë‹¤."""
    if not os.path.exists(criteria_path):
        print(f"WARNING: í‰ê°€ ê¸°ì¤€í‘œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {criteria_path}")
        return []
    
    print(f"INFO: í‰ê°€ ê¸°ì¤€í‘œ ë¡œë”©: {criteria_path}")
    
    with open(criteria_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸” íŒŒì‹±
    evaluation_items = []
    
    # í…Œì´ë¸” ë¼ì¸ ì°¾ê¸°
    lines = content.split('\n')
    table_started = False
    
    for line in lines:
        # í…Œì´ë¸” í—¤ë” ì°¾ê¸°
        if '| í‰ê°€ë¶€ë¬¸ |' in line:
            table_started = True
            continue
        
        # í…Œì´ë¸” êµ¬ë¶„ì„  ê±´ë„ˆë›°ê¸°
        if table_started and '---' in line:
            continue
        
        if table_started and line.strip().startswith('|') and '---' not in line:
            parts = [part.strip() for part in line.split('|')]
            if len(parts) >= 4:
                category = parts[1].replace('**', '').strip()
                topic = parts[2].replace('**', '').strip()
                criteria = parts[3].replace('**', '').strip()
                
                # ì†Œê³„, ì´ê³„, ë¹ˆ í–‰ ì œì™¸
                if (category and topic and criteria and 
                    'ì†Œê³„' not in category and 'ì´ê³„' not in category and
                    category != '' and topic != '' and criteria != ''):
                    evaluation_items.append({
                        "ëŒ€ë¶„ë¥˜": category,
                        "topic": topic,
                        "criteria": criteria
                    })
        
        # í…Œì´ë¸”ì´ ëë‚¬ëŠ”ì§€ í™•ì¸ (ë¹ˆ ì¤„ì´ë‚˜ ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘)
        elif table_started and not line.strip().startswith('|') and line.strip() != '':
            # ë‹¤ë¥¸ ì„¹ì…˜ ì‹œì‘ì¸ì§€ í™•ì¸
            if line.strip().startswith('##') or line.strip().startswith('---'):
                break
    
    print(f"INFO: {len(evaluation_items)}ê°œ í‰ê°€ í•­ëª©ì„ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
    return evaluation_items

def save_evaluation_report(proposal_name, report_content):
    """ì œì•ˆì„œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    proposal_base_name = os.path.splitext(proposal_name)[0]
    filename = f"{proposal_base_name}_evaluation_report_{timestamp}.txt"
    filepath = os.path.join(OUTPUT_DIR, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write("="*80 + "\n")
        f.write("ì œì•ˆì„œ í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(f"ì œì•ˆì„œëª…: {proposal_name}\n")
        f.write(f"ìƒì„±ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("="*80 + "\n")
        f.write("ìµœì¢… í‰ê°€ ë³´ê³ ì„œ\n")
        f.write("="*80 + "\n")
        f.write(report_content)
        f.write("\n\n" + "="*80 + "\n")
        f.write("ë³´ê³ ì„œ ë\n")
        f.write("="*80 + "\n")
    
    print(f"[SAVED] í‰ê°€ ë³´ê³ ì„œê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {filepath}")
    return filepath

#==============================================================


def get_llm_model():
    """í™˜ê²½ë³€ìˆ˜ì— ë”°ë¼ LLM ëª¨ë¸ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    model_type = os.getenv('LLM_TYPE', 'local').lower()
    
    if model_type == 'local':
        # ë¡œì»¬ Ollama ëª¨ë¸
        model_name = os.getenv('LOCAL_MODEL_NAME', 'llama3.2')
        base_url = os.getenv('OLLAMA_BASE_URL', 'http://localhost:11434')
        print(f"INFO: ë¡œì»¬ LLM ì‚¬ìš© - ëª¨ë¸: {model_name}, URL: {base_url}")
        return LLM(model=f"ollama/{model_name}", base_url=base_url)
    
    elif model_type == 'huggingface':
        # HuggingFace Hub ëª¨ë¸
        model_name = os.getenv('HF_MODEL_NAME', 'meta-llama/Meta-Llama-3-8B-Instruct')
        api_key = os.getenv('HUGGINGFACEHUB_API_TOKEN')
        if not api_key:
            raise ValueError("HUGGINGFACEHUB_API_TOKEN í™˜ê²½ë³€ìˆ˜ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print(f"INFO: HuggingFace LLM ì‚¬ìš© - ëª¨ë¸: {model_name}")
        return LLM(model=f"huggingface/{model_name}", api_key=api_key)
    
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” LLM íƒ€ì…ì…ë‹ˆë‹¤: {model_type}. 'local' ë˜ëŠ” 'huggingface'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")

# 2. CrewAI Agent ë° í”„ë¡œì„¸ìŠ¤ ì •ì˜
# =================================================================

async def main():
    """
    ë©”ì¸ í•¨ìˆ˜: ì œì•ˆì„œ ìë™ í‰ê°€ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰

    ì „ì²´ íë¦„:
    1. RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸, ë²¡í„°ìŠ¤í† ì–´)
    2. Phase 1: ì‹¬ì‚¬ í•­ëª© ìë™ ë¶„ë¥˜ (Dispatcher Agent)
    3. Phase 2: ëŒ€ë¶„ë¥˜ë³„ ì „ë¬¸ê°€ Agentê°€ ë³‘ë ¬ í‰ê°€
    4. Phase 3: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (Reporting Agent)
    """
    print("\n" + "="*70)
    print("  ë™ì  Agent ìƒì„± ë° í‰ê°€ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("="*70)

    # --- [ì „ì œ] RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ---
    # ë©”ì¸ í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ì „, ë²¡í„°ìŠ¤í† ì–´ë¥¼ ë¨¼ì € ì¤€ë¹„í•©ë‹ˆë‹¤.
    global unified_vectorstore

    # ì œì•ˆì„œ íŒŒì¼ ê²€ìƒ‰ (.txt, .html ë“±)
    proposal_files = glob.glob(os.path.join(PROPOSAL_DIR, "*.txt"))
    proposal_files.extend(glob.glob(os.path.join(PROPOSAL_DIR, "*.html")))

    # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not proposal_files:
        print("âŒ ì˜¤ë¥˜: ì œì•ˆì„œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   - ì œì•ˆì„œ ë””ë ‰í† ë¦¬: {PROPOSAL_DIR}")
        return

    if not os.path.exists(RFP_PATH):
        print("âŒ ì˜¤ë¥˜: RFP íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        print(f"   - RFP ê²½ë¡œ: {RFP_PATH}")
        return

    print(f"\nâœ… ì œì•ˆì„œ íŒŒì¼ {len(proposal_files)}ê°œ ë°œê²¬:")
    for pf in proposal_files:
        print(f"   - {os.path.basename(pf)}")

    # RAG ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ì„ë² ë”© ëª¨ë¸, ChromaDB)
    embedding_model, chroma_client = initialize_rag_components()

    # í†µí•© ë²¡í„°ìŠ¤í† ì–´ ìƒì„± (ì œì•ˆì„œ + RFP + ì‚¬ë‚´ì •ë³´)
    unified_vectorstore = create_unified_vectorstore(
        proposal_files=proposal_files,
        rfp_path=RFP_PATH,
        internal_data_dir=INTERNAL_DATA_DIR,  # ì‚¬ë‚´ ì •ë³´ ë””ë ‰í† ë¦¬ ì¶”ê°€!
        embedding_model=embedding_model,
        chroma_client=chroma_client
    )
    # --- RAG ì´ˆê¸°í™” ì™„ë£Œ ---

    # ë™ì ìœ¼ë¡œ í‰ê°€ ê¸°ì¤€ ë¡œë“œ
    unstructured_evaluation_items = load_evaluation_criteria(EVALUATION_CRITERIA_PATH)
    
    if not unstructured_evaluation_items:
        print("ERROR: í‰ê°€ ê¸°ì¤€ì„ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    # LLM ì´ˆê¸°í™”
    llm = get_llm_model()

    # =================================================================
    # Phase 1: Dispatcherê°€ ëŒ€ë¶„ë¥˜ë¥¼ ìŠ¤ìŠ¤ë¡œ ì°¾ì•„ë‚´ê³  í•­ëª© ë¶„ë¥˜
    # =================================================================
    # ëª©ì : ë¹„ì •í˜• ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ 'ëŒ€ë¶„ë¥˜' ê¸°ì¤€ìœ¼ë¡œ ìë™ ê·¸ë£¹í™”
    # ì˜ˆ: {"ê¸°ìˆ ": [...], "ê´€ë¦¬": [...], "ê°€ê²©": [...]}
    print("\n" + "="*70)
    print("  [Phase 1] ì‹¬ì‚¬ í•­ëª© ìë™ ë¶„ë¥˜")
    print("="*70)
    
    dispatcher_agent = Agent(
        role="í‰ê°€ í•­ëª© ìë™ ë¶„ë¥˜ ë° ê·¸ë£¹í™” ì „ë¬¸ê°€",
        goal="ì£¼ì–´ì§„ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ì—ì„œ 'ëŒ€ë¶„ë¥˜'ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ëª¨ë“  í•­ëª©ì„ ê·¸ë£¹í™”í•˜ì—¬ JSONìœ¼ë¡œ ë°˜í™˜",
        backstory="ë‹¹ì‹ ì€ ë³µì¡í•œ ëª©ë¡ì„ ë°›ì•„ì„œ ì£¼ìš” ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ê³  êµ¬ì¡°í™”í•˜ëŠ” ë° ë§¤ìš° ë›°ì–´ë‚œ ëŠ¥ë ¥ì„ ê°€ì¡ŒìŠµë‹ˆë‹¤.",
        llm=llm,
        verbose=True
    )

    items_as_string = json.dumps(unstructured_evaluation_items, ensure_ascii=False)
    
    dispatcher_task = Task(
        description=f"""ì•„ë˜ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ 'ëŒ€ë¶„ë¥˜' í‚¤ ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•´ì£¼ì„¸ìš”.
        [ì „ì²´ ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸]: {items_as_string}
        ê²°ê³¼ JSONì˜ keyëŠ” ë¦¬ìŠ¤íŠ¸ì— ì¡´ì¬í•˜ëŠ” 'ëŒ€ë¶„ë¥˜'ì˜ ì´ë¦„ì´ì–´ì•¼ í•©ë‹ˆë‹¤.
        ê° í•­ëª©ì˜ 'ëŒ€ë¶„ë¥˜', 'topic', 'criteria' í‚¤ì™€ ê°’ì„ ëª¨ë‘ ê·¸ëŒ€ë¡œ ìœ ì§€í•´ì•¼ í•©ë‹ˆë‹¤.
        """,
        expected_output="JSON ê°ì²´. ê° keyëŠ” ì‹¬ì‚¬ í•­ëª© ë¦¬ìŠ¤íŠ¸ì— ìˆë˜ 'ëŒ€ë¶„ë¥˜'ì´ë©°, valueëŠ” í•´ë‹¹ ëŒ€ë¶„ë¥˜ì— ì†í•˜ëŠ” í•­ëª© ê°ì²´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê° ê°ì²´ëŠ” ì›ë³¸ì˜ ëª¨ë“  í‚¤-ê°’ì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.",
        agent=dispatcher_agent
    )

    dispatcher_crew = Crew(agents=[dispatcher_agent], tasks=[dispatcher_task], verbose=False)
    categorization_result = dispatcher_crew.kickoff()
    
    try:
        # LLMì´ ìƒì„±í•œ ê²°ê³¼ë¬¼ì—ì„œ JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
        raw_result = str(categorization_result.raw)
        start_idx = raw_result.find('{')
        end_idx = raw_result.rfind('}') + 1
        
        if start_idx != -1 and end_idx > start_idx:
            json_string = raw_result[start_idx:end_idx]
            categorized_items = json.loads(json_string)
            print("[SUCCESS] í•­ëª© ë¶„ë¥˜ ì™„ë£Œ. ë°œê²¬ëœ ëŒ€ë¶„ë¥˜:")
            for category, items in categorized_items.items():
                print(f"  - {category}: {len(items)}ê°œ í•­ëª©")
        else:
            raise ValueError("JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    except (json.JSONDecodeError, ValueError) as e:
        print(f"[ERROR] í•­ëª© ë¶„ë¥˜ ì‹¤íŒ¨: {e}")
        print(f"   - ì›ë³¸ ê²°ê³¼: {categorization_result.raw}")
        # í´ë°±: ì›ë³¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        categorized_items = {}
        for item in unstructured_evaluation_items:
            category = item['ëŒ€ë¶„ë¥˜']
            if category not in categorized_items:
                categorized_items[category] = []
            categorized_items[category].append(item)
        print(f"[FALLBACK] ìˆ˜ë™ ë¶„ë¥˜ ì™„ë£Œ: {len(categorized_items)}ê°œ ëŒ€ë¶„ë¥˜")

    # =================================================================
    # Phase 2: ëŒ€ë¶„ë¥˜ ê°œìˆ˜ë§Œí¼ ë™ì ìœ¼ë¡œ Agentë¥¼ ìƒì„±í•˜ê³  ë³‘ë ¬ í‰ê°€
    # =================================================================
    # ëª©ì :
    # - ê° ëŒ€ë¶„ë¥˜(ê¸°ìˆ , ê´€ë¦¬, ê°€ê²© ë“±)ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì  ìƒì„±
    # - ê° ì œì•ˆì„œë¥¼ ìˆœíšŒí•˜ë©° ëª¨ë“  ì‹¬ì‚¬ í•­ëª©ì„ ë³‘ë ¬ í‰ê°€
    # - RAGë¥¼ í†µí•´ ì œì•ˆì„œì—ì„œ ê´€ë ¨ ë‚´ìš©ì„ ìë™ ì¶”ì¶œí•˜ì—¬ í‰ê°€ ê·¼ê±°ë¡œ í™œìš©
    print("\n" + "="*70)
    print("  [Phase 2] ë°œê²¬ëœ ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±í•˜ì—¬ ë³‘ë ¬ í‰ê°€í•©ë‹ˆë‹¤")
    print("="*70)

    # ëª¨ë“  ì œì•ˆì„œ íŒŒì¼ì— ëŒ€í•´ í‰ê°€ë¥¼ ë°˜ë³µí•©ë‹ˆë‹¤.
    for proposal_path in proposal_files:
        proposal_name = os.path.basename(proposal_path)
        print(f"\n\n{'='*20} [{proposal_name}] í‰ê°€ ì‹œì‘ {'='*20}")

        # ì œì•ˆì„œë³„ë¡œ Agentì™€ Task ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
        specialist_agents = []  # ì „ë¬¸ê°€ Agent ë¦¬ìŠ¤íŠ¸
        evaluation_tasks = []   # í‰ê°€ Task ë¦¬ìŠ¤íŠ¸

        # ëŒ€ë¶„ë¥˜ë³„ë¡œ ì „ë¬¸ê°€ Agentë¥¼ ë™ì  ìƒì„±
        for category, items in categorized_items.items():
            # ëŒ€ë¶„ë¥˜ë³„ ì „ë¬¸ê°€ Agent ìƒì„± (ì˜ˆ: "ê¸°ìˆ  ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€")
            specialist_agent = Agent(
                role=f"'{category}' ë¶€ë¬¸ ì „ë¬¸ í‰ê°€ê´€",
                goal=f"'{proposal_name}' ì œì•ˆì„œì˜ '{category}' ë¶€ë¬¸ì„ ì „ë¬¸ì ìœ¼ë¡œ í‰ê°€",
                backstory=f"ë‹¹ì‹ ì€ '{category}' ë¶„ì•¼ ìµœê³ ì˜ ì „ë¬¸ê°€ë¡œì„œ, ì£¼ì–´ì§„ ê´€ë ¨ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¬ì‚¬ ê¸°ì¤€ì— ë”°ë¼ ì œì•ˆì„œë¥¼ ëƒ‰ì² í•˜ê²Œ ë¶„ì„í•˜ê³  í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤.",
                llm=llm,
                verbose=True
            )
            specialist_agents.append(specialist_agent)

            # í•´ë‹¹ ëŒ€ë¶„ë¥˜ì˜ ëª¨ë“  ì‹¬ì‚¬ í•­ëª©ì— ëŒ€í•œ Task ìƒì„±
            for item in items:
                # RAGë¥¼ í†µí•´ ì œì•ˆì„œì—ì„œ ê´€ë ¨ ë‚´ìš© ê²€ìƒ‰
                # - ë²¡í„°ìŠ¤í† ì–´ì—ì„œ í† í”½ê³¼ ìœ ì‚¬í•œ ë‚´ìš©ì„ ìë™ìœ¼ë¡œ ì°¾ì•„ì˜´
                context = get_context_for_topic(proposal_name, item['topic'])

                # í‰ê°€ Task ìƒì„±
                task = Task(
                    description=f"ì œì•ˆì„œ '{proposal_name}'ì˜ '{item.get('topic', 'N/A')}' í•­ëª©ì„ í‰ê°€í•˜ì‹œì˜¤.\n\nì‹¬ì‚¬ê¸°ì¤€: {item.get('criteria', 'N/A')}\n\nê´€ë ¨ë‚´ìš©:\n{context}\n\ní‰ê°€ì ìˆ˜(1-100), ìš”ì•½, ê·¼ê±°ë¥¼ í¬í•¨í•œ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹œì˜¤.",
                    expected_output=f"í‰ê°€ì ìˆ˜(1-100), ìš”ì•½, ê·¼ê±°ë¥¼ í¬í•¨í•œ '{item.get('topic', 'N/A')}' í‰ê°€ë³´ê³ ì„œ",
                    agent=specialist_agent
                )
                evaluation_tasks.append(task)

        # Taskê°€ ì—†ìœ¼ë©´ ë‹¤ìŒ ì œì•ˆì„œë¡œ
        if not evaluation_tasks:
            print("âš  í‰ê°€í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            continue

        # Crew êµ¬ì„± ë° ë³‘ë ¬ í‰ê°€ ì‹¤í–‰
        # - ì—¬ëŸ¬ ì „ë¬¸ê°€ Agentê°€ ê°ìì˜ Taskë¥¼ ë™ì‹œì— ìˆ˜í–‰
        print(f"\nâ³ {len(evaluation_tasks)}ê°œ í‰ê°€ í•­ëª©ì„ ì²˜ë¦¬ ì¤‘...")
        evaluation_crew = Crew(
            agents=specialist_agents,
            tasks=evaluation_tasks,
            verbose=False  # ì¶œë ¥ ê°„ì†Œí™”
        )
        final_results = await evaluation_crew.kickoff_async()  # ë¹„ë™ê¸° ë³‘ë ¬ ì‹¤í–‰

        # =================================================================
        # Phase 3: ìµœì¢… ë³´ê³ ì„œ ì‘ì„± (Reporting Agent)
        # =================================================================
        # ëª©ì :
        # - ëª¨ë“  ê°œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬ í•˜ë‚˜ì˜ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±
        # - ì œì•ˆì„œ ì „ì²´ì— ëŒ€í•œ ì¢…í•© í‰ê°€, ê°•ì /ì•½ì  ë¶„ì„, ìµœì¢… ì ìˆ˜ ì œì‹œ
        print(f"\n{'='*70}")
        print(f"  [Phase 3] [{proposal_name}] ìµœì¢… ë³´ê³ ì„œ ì‘ì„±")
        print(f"{'='*70}")

        # ê°œë³„ í‰ê°€ ë³´ê³ ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
        individual_reports = "\n\n".join([str(result) for result in final_results])

        # ìµœì¢… ë³´ê³ ì„œ ì‘ì„± Agent ìƒì„±
        reporting_agent = Agent(
            role="ìˆ˜ì„ í‰ê°€ ë¶„ì„ê°€",
            goal="ì—¬ëŸ¬ ê°œì˜ ê°œë³„ í‰ê°€ ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬, ê²½ì˜ì§„ì´ ì˜ì‚¬ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆë„ë¡ í•˜ë‚˜ì˜ ì™„ì„±ëœ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±",
            backstory="ë‹¹ì‹ ì€ ì—¬ëŸ¬ ë¶€ì„œì˜ ë³´ê³ ë¥¼ ì·¨í•©í•˜ì—¬ í•µì‹¬ë§Œ ìš”ì•½í•˜ê³ , ì „ì²´ì ì¸ ê´€ì ì—ì„œ ê°•ì ê³¼ ì•½ì ì„ ë¶„ì„í•˜ì—¬ ìµœì¢… ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ëŠ” ë° ë§¤ìš° ëŠ¥ìˆ™í•©ë‹ˆë‹¤.",
            llm=llm,
            verbose=True
        )

        # ìµœì¢… ë³´ê³ ì„œ ì‘ì„± Task ìƒì„±
        reporting_task = Task(
            description=f"""'{proposal_name}' ì œì•ˆì„œì˜ ê°œë³„ í‰ê°€ë³´ê³ ì„œë¥¼ ì¢…í•©í•˜ì—¬ ìµœì¢…ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì‹œì˜¤.

ê°œë³„ë³´ê³ ì„œ:
{individual_reports}

ìœ„ ë³´ê³ ì„œë“¤ì„ ëª¨ë‘ ì¢…í•©í•˜ì—¬, '{proposal_name}'ì— ëŒ€í•œ ìµœì¢… í‰ê°€ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.
ë³´ê³ ì„œì—ëŠ” ë‹¤ìŒ ë‚´ìš©ì´ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤:

1. **ì„œë¡ **: í‰ê°€ ê°œìš” ë° í‰ê°€ ë°©ë²•ë¡ 
2. **ì¢…í•© ì˜ê²¬**: ì œì•ˆì„œì˜ í•µì‹¬ì ì¸ ê°•ì ê³¼ ì•½ì ì— ëŒ€í•œ ì´í‰
3. **í•­ëª©ë³„ ìƒì„¸ ë¶„ì„**: 
   - ê°€ê²© í‰ê°€ (35ì  ë§Œì )
   - íšŒì‚¬ ì•ˆì •ì„± ë° ê¸°ìˆ ë ¥ (20ì  ë§Œì )  
   - í”„ë¡œì íŠ¸ ê²½í—˜ ë° ê´€ë¦¬ (35ì  ë§Œì )
   - êµìœ¡ ë° ê¸°ìˆ ì§€ì› (15ì  ë§Œì )
4. **ì„¸ë¶€ í‰ê°€ ë‚´ìš©**: ê° í•­ëª©ë³„ êµ¬ì²´ì ì¸ í‰ê°€ ê·¼ê±°ì™€ ì ìˆ˜
5. **ìµœì¢… ì ìˆ˜**: 100ì  ë§Œì  ê¸°ì¤€ ì´ì 
6. **ì¶”ì²œ ì‚¬í•­**: ê°œì„ ì´ í•„ìš”í•œ ë¶€ë¶„ê³¼ ìš°ìˆ˜í•œ ë¶€ë¶„ì— ëŒ€í•œ êµ¬ì²´ì  ì œì•ˆ
7. **ê²°ë¡ **: ìµœì¢… ì˜ì‚¬ê²°ì •ì„ ìœ„í•œ ì¢…í•©ì  íŒë‹¨

ê° í•­ëª©ì— ëŒ€í•´ êµ¬ì²´ì ì´ê³  ìƒì„¸í•œ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.""",
            expected_output="ì„œë¡ , ì¢…í•© ì˜ê²¬, í•­ëª©ë³„ ìƒì„¸ ë¶„ì„, ì„¸ë¶€ í‰ê°€ ë‚´ìš©, ìµœì¢… ì ìˆ˜, ì¶”ì²œ ì‚¬í•­, ê²°ë¡ ì´ í¬í•¨ëœ ì™„ì„±ëœ í˜•íƒœì˜ ìµœì¢… í‰ê°€ ë³´ê³ ì„œ",
            agent=reporting_agent
        )

        # ìµœì¢… ë³´ê³ ì„œ ìƒì„± Crew ì‹¤í–‰
        reporting_crew = Crew(agents=[reporting_agent], tasks=[reporting_task], verbose=False)
        final_comprehensive_report = reporting_crew.kickoff()

        print(f"\n\n[FINAL REPORT] [{proposal_name}] ìµœì¢… ì¢…í•© í‰ê°€ ë³´ê³ ì„œ\n==========================================")
        print(final_comprehensive_report.raw)
        print("==========================================\n")
        
        # í‰ê°€ ë³´ê³ ì„œë¥¼ íŒŒì¼ë¡œ ì €ì¥
        save_evaluation_report(proposal_name, final_comprehensive_report.raw)


def run_main():
    """ë™ê¸°ì ìœ¼ë¡œ main í•¨ìˆ˜ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    asyncio.run(main())

if __name__ == '__main__':
    run_main()
